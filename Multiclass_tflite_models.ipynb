{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "by3j7wNK_o6T"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries that are being used below\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, RNN, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVcuAo2duzaC",
        "outputId": "648a26b3-fa28-4e9a-bb9d-5368b1ffc4dd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "26Fy5sgvH6W4"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"Multi_labeled_CAN_dataset.csv\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "data['ID'] = label_encoder.fit_transform(data['ID'])\n",
        "data['DATA'] = label_encoder.fit_transform(data['DATA'])\n",
        "X = data[['Timestamp', 'ID', 'DLC', 'DATA']].values\n",
        "y = data['Class'].values\n",
        "\n",
        "# Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HSaOHLyYbadL"
      },
      "outputs": [],
      "source": [
        "# Reshape the input data to have a temporal dimension\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LuH8_1KPbCp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1VumJR9HwDq",
        "outputId": "90c4c3e3-135d-47ad-8b2d-b1f5d749e566"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4152095, 1, 4)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q3yRLnDW_Krw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the path to the saved Keras model file\n",
        "model_file_path = 'Multiclass_model.h5'\n",
        "\n",
        "# Load the saved Keras model\n",
        "model = load_model(model_file_path)\n",
        "\n",
        "# Now you can use the `new_model` variable to make predictions or perform other operations\n",
        "# For example:\n",
        "# y_pred = new_model.predict(X_test)\n",
        "# print(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaJDqCWuwsJR",
        "outputId": "841696da-2064-4a72-d8d3-c16a417e4d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m14417/14417\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.8812 - loss: 0.3399\n",
            "Test Accuracy: 88.04%\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for memory profiling and timing\n",
        "import os\n",
        "# import timeit\n",
        "\n",
        "# Predict accuracy on test dataset\n",
        "accuracy = model.evaluate(X_test, y_test)\n",
        "test_accuracy_percentage = accuracy[1] * 100  # Convert accuracy to percentage\n",
        "print(\"Test Accuracy:\", \"{:.2f}%\".format(test_accuracy_percentage))  # Display accuracy as percentage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCgVGSXdZOCX",
        "outputId": "d4418a21-bf42-48b8-e65c-c25cf9c5d639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "non_quantized model is 200440 bytes\n"
          ]
        }
      ],
      "source": [
        "# Measure memory usage of the model\n",
        "non_quantized_model_size = os.path.getsize('Multiclass_model.h5')\n",
        "print(\"non_quantized model is %d bytes\" % non_quantized_model_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QljXJbNlw4Yv",
        "outputId": "536866fb-1283-42dd-fcb8-1eadbf3e36db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Prediction Time for a Single Data Point: 102.95 ms\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "# Select a single data point from the test dataset for prediction\n",
        "single_test_data = X_test[0].reshape(1, 1, X.shape[2])  # Reshape for single data point\n",
        "\n",
        "# Measure prediction time\n",
        "start_time = timer()\n",
        "p = model.predict(single_test_data)\n",
        "end_time = timer()\n",
        "prediction_time = (end_time - start_time) * 1000\n",
        "\n",
        "print(\"Prediction Time for a Single Data Point:\", \"{:.2f} ms\".format(prediction_time))  # Display prediction time in milliseconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgG7I5WA6QxS",
        "outputId": "3538172f-691a-46de-f182-308cba3687b0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'float32'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1NLs4Bn6PYT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOaBXrAL18LJ"
      },
      "source": [
        "## **TFLite Default**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7Cu7AJbYd8pR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\routs\\AppData\\Local\\Temp\\tmp3c0cpix9\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\routs\\AppData\\Local\\Temp\\tmp3c0cpix9\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\routs\\AppData\\Local\\Temp\\tmp3c0cpix9'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1, 4), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  1699618332624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618331664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618333008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618333776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'model' is your original TensorFlow/Keras model\n",
        "# Convert the model to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS      # enable TensorFlow ops.\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "with open(\"c_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zH0B1Xek-rR"
      },
      "source": [
        "## **tflite default delay**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAFgGaC32Z3f",
        "outputId": "51712be8-1bcc-498a-8633-bc9f4a544ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite Model Prediction Time for a Single Data Point: 1.07 ms\n"
          ]
        }
      ],
      "source": [
        "# Load the TFLite model\n",
        "c_model = tf.lite.Interpreter('c_model.tflite')\n",
        "c_model.allocate_tensors()\n",
        "\n",
        "test_image = np.expand_dims(X_test[0], axis=0).astype(np.float32)\n",
        "\n",
        "# Get indexes of the input and output tensors\n",
        "c_model_input_index = c_model.get_input_details()[0][\"index\"]\n",
        "c_model_output_index = c_model.get_output_details()[0][\"index\"]\n",
        "\n",
        "start1 = timer()\n",
        "c_model.set_tensor(c_model_input_index, test_image)\n",
        "c_model.invoke()\n",
        "predictions_default = c_model.get_tensor(c_model_output_index)\n",
        "end1 = timer()\n",
        "\n",
        "prediction_default_time = (end1-start1) * 1000  ## in ms\n",
        "print(\"TFLite Model Prediction Time for a Single Data Point:\", \"{:.2f} ms\".format(prediction_default_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3JqyUtrmOWN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq17Ud5Nk3Ii"
      },
      "source": [
        "## **TFlite default accuracy**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NrwNF7QenXd",
        "outputId": "f5b4619c-47e5-4d35-b647-c950e2854f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite Model Testing Accuracy: 88.04%\n"
          ]
        }
      ],
      "source": [
        "# # Evaluate testing accuracy of the TFLite model\n",
        "correct_count = 0\n",
        "total_count = len(X_test)\n",
        "\n",
        "# def f1(p):\n",
        "#   return 1 if p >= 0.5 else 0\n",
        "\n",
        "for i, x_value in enumerate(X_test):\n",
        "    x_value_tensor = tf.convert_to_tensor(x_value, dtype=np.float32)\n",
        "    x_value_tensor = np.expand_dims(x_value_tensor, axis=0)\n",
        "    c_model.set_tensor(c_model_input_index, x_value_tensor)\n",
        "    c_model.invoke()\n",
        "    c_model_prediction = c_model.get_tensor(c_model_output_index)[0]\n",
        "\n",
        "    # Compare prediction with ground truth label\n",
        "    if (np.argmax(c_model_prediction) == y_test[i]):\n",
        "        correct_count += 1\n",
        "\n",
        "accuracy_percentage = (correct_count / total_count) * 100\n",
        "\n",
        "# Calculate and print testing accuracy of the TFLite model\n",
        "\n",
        "print(\"TFLite Model Testing Accuracy:\", \"{:.2f}%\".format(accuracy_percentage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i9Pw4oc2ezf",
        "outputId": "9fb58f56-f269-445e-a882-c4eed12193f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "floating point 32 quantized model is 68268 bytes\n"
          ]
        }
      ],
      "source": [
        "# Measure memory usage of the TFLite model\n",
        "c_model_default_size = os.path.getsize(\"c_model.tflite\")\n",
        "print(\"floating point 32 quantized model is %d bytes\" % c_model_default_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J_w8J4I53Hb"
      },
      "source": [
        "## **TFLite with float16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "eF14CIr37nxm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\routs\\AppData\\Local\\Temp\\tmp8w61spof\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\routs\\AppData\\Local\\Temp\\tmp8w61spof\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\routs\\AppData\\Local\\Temp\\tmp8w61spof'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1, 4), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  1699618332624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618331664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618333008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618333776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "# Create a TFLite converter from the Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Apply optimizations and specify supported ops\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable TensorFlow Lite ops.\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS      # Enable TensorFlow ops\n",
        "]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# Disable experimental tensor list lowering\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to a .tflite file\n",
        "with open('model_fp16.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# print(\"TFLite model conversion completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKXosnPuni5E"
      },
      "source": [
        "## **fp16 delay**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih7Lr4btDSef",
        "outputId": "99af7c27-8160-4683-c590-b197624cf22a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite Model Prediction Time for a Single Data Point: 0.99 ms\n"
          ]
        }
      ],
      "source": [
        "# Load the TFLite model\n",
        "fp16_model = tf.lite.Interpreter('model_fp16.tflite')\n",
        "fp16_model.allocate_tensors()\n",
        "\n",
        "# Get indexes of the input and output tensors\n",
        "fp16_model_input_index = fp16_model.get_input_details()[0][\"index\"]\n",
        "fp16_model_output_index = fp16_model.get_output_details()[0][\"index\"]\n",
        "\n",
        "start2 = timer()\n",
        "fp16_model.set_tensor(fp16_model_input_index, test_image)\n",
        "fp16_model.invoke()\n",
        "predictions_fp16 = fp16_model.get_tensor(fp16_model_output_index)\n",
        "end2 = timer()\n",
        "prediction_fp16_time = (end2-start2) * 1000  ## in ms\n",
        "# predictions\n",
        "\n",
        "print(\"TFLite Model Prediction Time for a Single Data Point:\", \"{:.2f} ms\".format(prediction_fp16_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mei1vx8am8IU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqDXP4TUneMV"
      },
      "source": [
        "## **fp16 accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHbJIv43h0Pf",
        "outputId": "2797b7b2-7b40-4547-c688-043820050b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite fp16 Model Testing Accuracy: 88.01%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate testing accuracy of the TFLite model\n",
        "correct_count = 0\n",
        "\n",
        "for i, x_value in enumerate(X_test):\n",
        "    x_value_tensor = tf.convert_to_tensor(x_value, dtype=np.float32)\n",
        "    x_value_tensor = np.expand_dims(x_value_tensor, axis=0)\n",
        "    fp16_model.set_tensor(fp16_model_input_index, x_value_tensor)\n",
        "    fp16_model.invoke()\n",
        "    fp16_model_prediction = fp16_model.get_tensor(fp16_model_output_index)[0]\n",
        "\n",
        "   \n",
        "    # Compare prediction with ground truth label\n",
        "    if (np.argmax(fp16_model_prediction) == y_test[i]):\n",
        "        correct_count += 1\n",
        "\n",
        "accuracy_percentage = (correct_count / total_count) * 100\n",
        "\n",
        "# Calculate and print testing accuracy of the TFLite model\n",
        "\n",
        "print(\"TFLite fp16 Model Testing Accuracy:\", \"{:.2f}%\".format(accuracy_percentage))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySAAmm_OnZvE"
      },
      "source": [
        "## **fp 16 tflite memory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVo7iiFIER-9",
        "outputId": "93a832d5-d6c4-4671-e1ce-5ffe355d24bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "floating point 16 quantized model is 41396 bytes\n"
          ]
        }
      ],
      "source": [
        "# Measure memory usage of the TFLite model\n",
        "fp16_quantized_model_size = os.path.getsize(\"model_fp16.tflite\")\n",
        "print(\"floating point 16 quantized model is %d bytes\" % fp16_quantized_model_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyfT9tGKBuW"
      },
      "source": [
        "# **TFLite with int8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDgl99hGKUI-",
        "outputId": "1da85a06-7c9e-4c26-d004-ae87cfbbd334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\routs\\AppData\\Local\\Temp\\tmpt8xgnhlp\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\routs\\AppData\\Local\\Temp\\tmpt8xgnhlp\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at 'C:\\Users\\routs\\AppData\\Local\\Temp\\tmpt8xgnhlp'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1, 4), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  1699618332624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618331664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618333008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618330320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  1699618333776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\routs\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def representative_dataset():\n",
        "  for value in X_test:\n",
        "     yield [np.array(value, dtype=np.float32, ndmin=3)]\n",
        "\n",
        "# Create a TFLite converter from the Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Apply optimizations and specify supported ops\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS, # enable TensorFlow ops.\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS_INT8\n",
        "]\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Disable experimental tensor list lowering\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "\n",
        "# Convert the model to TensorFlow Lite format\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to a .tflite file\n",
        "with open('model_int8.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# print(\"TFLite model conversion completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg8yCBaNoNUf"
      },
      "source": [
        "## **int8 tflite delay**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDmJtmuyG25V",
        "outputId": "3b89c177-0cc1-4bcc-a6b5-c20953345f2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite Model Prediction Time for a Single Data Point: 0.89 ms\n"
          ]
        }
      ],
      "source": [
        "# Load the TFLite model\n",
        "int8_model = tf.lite.Interpreter('model_int8.tflite')\n",
        "int8_model.allocate_tensors()\n",
        "\n",
        "test_image = np.expand_dims(X_test[0], axis=0).astype(np.float32)\n",
        "\n",
        "# Get indexes of the input and output tensors\n",
        "int8_model_input_index = int8_model.get_input_details()[0][\"index\"]\n",
        "int8_model_output_index = int8_model.get_output_details()[0][\"index\"]\n",
        "\n",
        "start3 = timer()\n",
        "int8_model.set_tensor(int8_model_input_index, test_image)\n",
        "int8_model.invoke()\n",
        "predictions = int8_model.get_tensor(int8_model_output_index)\n",
        "end3 = timer()\n",
        "prediction_int8_time = (end3 -start3) * 1000  ## in ms\n",
        "# predictions\n",
        "\n",
        "print(\"TFLite Model Prediction Time for a Single Data Point:\", \"{:.2f} ms\".format(prediction_int8_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llzgz5s-pLWi"
      },
      "source": [
        "## **int8 tflite aaccuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lff87RJwi7x0",
        "outputId": "85f0b29f-8b8f-4d8c-b4c6-45fdd5a1a0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TFLite int8 Model Testing Accuracy: 83.63%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# # Evaluate testing accuracy of the TFLite model\n",
        "correct_count = 0\n",
        "\n",
        "for i, x_value in enumerate(X_test):\n",
        "    x_value_tensor = tf.convert_to_tensor(x_value, dtype=np.float32)\n",
        "    x_value_tensor = np.expand_dims(x_value_tensor, axis=0)\n",
        "    int8_model.set_tensor(int8_model_input_index, x_value_tensor)\n",
        "    int8_model.invoke()\n",
        "    int8_model_prediction = int8_model.get_tensor(int8_model_output_index)[0]\n",
        "\n",
        "    \n",
        "    # Compare prediction with ground truth label\n",
        "    if (np.argmax(int8_model_prediction) == y_test[i]):\n",
        "        correct_count += 1\n",
        "\n",
        "accuracy_percentage = (correct_count / total_count) * 100\n",
        "\n",
        "# Calculate and print testing accuracy of the TFLite model\n",
        "\n",
        "print(\"TFLite int8 Model Testing Accuracy:\", \"{:.2f}%\".format(accuracy_percentage))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt8AVUvVpFNG"
      },
      "source": [
        "## **int8 tflite memory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljqA02jSKwR9",
        "outputId": "1d1afd3b-7e44-45c2-cf84-3f70c4a7d4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "integer point 8 quantized model is 40328 bytes\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# Measure memory usage of the TFLite model\n",
        "int8_quantized_model_size = os.path.getsize(\"model_int8.tflite\")\n",
        "print(\"integer point 8 quantized model is %d bytes\" % int8_quantized_model_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
